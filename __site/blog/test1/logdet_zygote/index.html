<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>Differentiate the log-determinant of a matrix on GPU using Zygote.jl</title>  
</head>
<body>
<header>
  <h1 style="color:#283747">Mathematics</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><h2 id="differentiate_the_log-determinant_of_a_matrix_on_gpu_using_zygotejl"><a href="#differentiate_the_log-determinant_of_a_matrix_on_gpu_using_zygotejl" class="header-anchor">Differentiate the log-determinant of a matrix on GPU using Zygote.jl</a></h2>
<h3 id="log-determinant_in_a_computational_graph"><a href="#log-determinant_in_a_computational_graph" class="header-anchor">Log-determinant in a computational graph</a></h3>
<p>Does the computation of log-determinant of a matrix ever occur in the computational graph of a neural network?</p>
<p>Surprisingly, it does. This arises in the context of Sparse Variational Gaussian Process &#40;SVGP&#41;.</p>
<p>In SVGP, our goal is to maximize a quantity known as the evidence lower bound &#40;ELBO&#41;. The ELBO comprises of two terms, which basically says:</p>
\[ \begin{align*}
    \text{ELBO} = \text{expected log likelihood term} - \text{KL divergence term}
    \end{align*}
\]
<p>One could check, e.g. <a href="https://towardsdatascience.com/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7">Wei Yi&#39;s excellent tutorial</a> or James Hensman&#39;s <a href="https://arxiv.org/pdf/1309.6835.pdf">Gaussian Process for big data</a> for the derivation of ELBO for SVGP. In short, the KL divergence term in ELBO is expressed as:</p>
\[ \begin{align*}
    \text{KL divergence term in ELBO} = {\color{grey}\frac{1}{2}\bigg[}\log\frac{1}{\det\Sigma} + {\color{grey}n + \mu\top\mu + \text{trace}(\Sigma)\bigg]}
    \end{align*}
\]
<p>&#40;\(\mu\) and \(\Sigma\) are mean and covariance of a variational distribution, and they are part of the model parameters in SVGP&#41; In this context, we will disregard the grey terms as they are easily handled by Zygote.</p>
<h3 id="differentiate_the_log-determinant_of_a_matrix_using_zygotejl"><a href="#differentiate_the_log-determinant_of_a_matrix_using_zygotejl" class="header-anchor">Differentiate the log-determinant of a matrix using Zygote.jl</a></h3>
<p>When \(\Sigma\) is stored in CPU memory, the term \(\log\det\Sigma\) involving the &#40;positive definite&#41; matrix \(\Sigma\) works fine with Zygote.</p>
<pre><code class="language-julia">using Zygote
X &#61; randn&#40;3,3&#41;; XᵀX &#61; X&#39;X
f&#40;x&#41; &#61; logdet&#40;x&#41;
@show gradient&#40;f, XᵀX&#41;&#91;1&#93;</code></pre>
<p>This gives:</p>
<pre><code class="language-julia">&#40;gradient&#40;f, XᵀX&#41;&#41;&#91;1&#93; &#61; &#91;0.8429186329377117 -0.4507909324777994 -0.7811665008998808; -0.45079093247779933 0.48173303692414393 0.47267755816965557; -0.7811665008998809 0.4726775581696556 1.261152638854635&#93;</code></pre>
<p>But interestingly, it does not work at all when \(\Sigma\) is stored in GPU memory.</p>
<pre><code class="language-julia">using CUDA
CUDA.allowscalar&#40;false&#41;
@show gradient&#40;f, cu&#40;XᵀX&#41;&#41;&#91;1&#93;</code></pre>
<p>This will return:</p>
<code>Error: Scalar indexing is disallowed.</code>
<p>Let&#39;s try to get around this, using the <a href="https://blogs.sas.com/content/iml/2012/10/31/compute-the-log-determinant-of-a-matrix.html">Cholesky decomposition to calculate the log determinant</a> of \(\Sigma\):</p>
<pre><code class="language-julia">cholesky_log_det&#40;X&#41; &#61; begin
    C &#61; cholesky&#40;X&#41;
    return 2*sum&#40;log.&#40;diag&#40;C.L&#41;&#41;&#41;
end
@show gradient&#40;cholesky_log_det, cu&#40;XᵀX&#41;&#41;&#91;1&#93;</code></pre>
<p>But this again gives:</p>
<code>Error: Scalar indexing is disallowed.</code>
<h3 id="customized_adjoint_to_the_rescue"><a href="#customized_adjoint_to_the_rescue" class="header-anchor">Customized adjoint to the rescue</a></h3>
<p>It turns out that <a href="https://statisticaloddsandends.wordpress.com/2018/05/24/derivative-of-log-det-x/">the derivative of the log-determinant of a matrix has a very simple formula</a>, i.e., for an invertible matrix \(Z\), we have that </p>
\[ \begin{align*}
    (\log\det Z)' = Z⁻ᵀ
    \end{align*}
\]
<p>Using this handy fact, let&#39;s go ahead and <a href="https://fluxml.ai/Zygote.jl/stable/adjoints/">make a customized adjoint</a>:</p>
<pre><code class="language-julia">using Zygote: @adjoint

function log_determinant&#40;Q::CuMatrix&#41;
    A &#61; cholesky&#40;Q&#41;
    return 2*sum&#40;log.&#40;diag&#40;A.L&#41;&#41;&#41;
end

@adjoint function log_determinant&#40;Q::CuMatrix&#41;
    # Q positive definite so Q &#61; LLᵀ by cholesky and thus Q⁻¹ &#61; L⁻ᵀL⁻¹
    # numerically stable way to invert a covariance matrix: https://mathoverflow.net/questions/9001/inverting-a-covariance-matrix-numerically-stable
    A &#61; cholesky&#40;Q&#41;
    L_inv &#61; inv&#40;A.L&#41;
    A_inv &#61; L_inv&#39;L_inv  
    return 2*sum&#40;log.&#40;diag&#40;A.L&#41;&#41;&#41;, △ -&gt; &#40;△ * A_inv&#39;, &#41;
end

@show gradient&#40;log_determinant, cu&#40;XᵀX&#41;&#41;&#91;1&#93;</code></pre>
<p>And now we have:</p>
<pre><code class="language-julia">&#40;gradient&#40;log_determinant, cu&#40;XᵀX&#41;&#41;&#41;&#91;1&#93; &#61; Float32&#91;0.8429185 -0.4507908 -0.78116626; -0.4507908 0.48173293 0.4726774; -0.78116626 0.4726774 1.2611524&#93;</code></pre>
<p>This works. A quick check with the CPU version&#39;s result shows that our adjoint is returning the correct gradient of the log determinant of \(\Sigma\).</p>
<p>A side note: You may have noticed the line <code>L_inv &#61; inv&#40;A.L&#41;</code>. Indeed, the inversion of a triangular matrix <code>A.L</code> still has quadratic time complexity, which is pretty darn slow for big matricies. Fortunately, in SVGP, the matrix \(\Sigma\), i.e. the input matrix <code>Q</code> above is defined using what&#39;s called the &quot;inducing points&quot;, which makes <code>Q</code> small. And because the inducing points is part of the model parameter of SVGP, you actually get to control the size of \(\Sigma\).</p>
<p>An adjoint example is <a href="https://discourse.julialang.org/t/zygote-meaning-of-adjoint-add-a-b-add-a-b/36707/4">here</a> by Pbellive.</p>
<p>Note: This post is done on Zygote version 0.6.61 and Julia version 1.9.</p>
<p><hr /> update: 4/4/2024</p>
<p>The following code using <code>ChainRulesCore</code> produces the same result.</p>
<pre><code class="language-julia">function log_determinant&#40;Q::CuMatrix&#41;
    A &#61; cholesky&#40;Q&#41;
    return 2*sum&#40;log.&#40;diag&#40;A.L&#41;&#41;&#41;
end

function ChainRulesCore.rrule&#40;::typeof&#40;log_determinant&#41;, Q::CuMatrix&#41;
    A &#61; cholesky&#40;Q&#41;
    L_inv &#61; inv&#40;A.L&#41;
    A_inv &#61; L_inv&#39; * L_inv
    function log_determinant_pullback&#40;R̄&#41;
        f̄ &#61; NoTangent&#40;&#41;
        Q̄ &#61; R̄ * A_inv&#39;
        return f̄, Q̄
    end
    return 2*sum&#40;log.&#40;diag&#40;A.L&#41;&#41;&#41;, log_determinant_pullback
end</code></pre>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: October 06, 2025.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
